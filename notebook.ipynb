{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#####   INF889E - Méthodes d'intelligence artificielle en bioinformatique   #####\n",
    "#####             Classification de VIH par zones géographiques             #####\n",
    "#################################################################################\n",
    "#####   Author: Riccardo Z******                                            #####\n",
    "#####   This program is partly inspired by the work presented in a class    #####\n",
    "#####   workshop by Dylan Lebatteux.                                        #####\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import functions\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from random import shuffle\n",
    "from progressbar import ProgressBar\n",
    "from Bio import SeqIO, pairwise2\n",
    "from Bio.motifs import create\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from dna_features_viewer import GraphicFeature, GraphicRecord\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####        IMPORTANT VARIABLES         #####\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scope of classification: if \"ALL\", classify by region globaly\n",
    "# If AFR, ASI, CAM, CRB, EUR, FSU, MEA, NAM, OCE or SAM, classify by country within this chosen region \n",
    "scope = \"AFR\"\n",
    "# Access path for the FASTA files (one file for each region)\n",
    "path = \"data/\" + \"full\"\n",
    "# Name of trained model when saving\n",
    "model_name = \"model.pkl\"\n",
    "# For sampling purposes: will process max n sequences for each target class\n",
    "n_samples = 100\n",
    "# Classification features as sum (false) of motifs or as frequency (true) of motifs\n",
    "freq = False\n",
    "# Elimination step for features selection\n",
    "step = 5\n",
    "# Number of features to select\n",
    "n_features = 100\n",
    "# Train / Test split ratio\n",
    "split_raito = 0.8\n",
    "# Dimensions for graphs (2D or 3D)\n",
    "n_components = 3\n",
    "# Set maximum number of incorrect records to analyse at the end\n",
    "max_incorrect = 10\n",
    "# Set the length k of the features based on k-mers\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####        DATA INITIALISATION         #####\n",
    "##############################################\n",
    "print(\"\\n         DATA INITIALISATION         \")\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will contain all the data rows, in the form of biopython seq_records objects\n",
    "data = []\n",
    "# Will contain a pair of target class -> number of data rows with this target class\n",
    "targets = {}\n",
    "# Process raw record label information into its annotations, then insert it into data\n",
    "# To update if the label of sequences in the FASTA files changes\n",
    "def add_record(record, target):\n",
    "    # Initialiation of the seq_record\n",
    "    header = record.id.split(\".\")\n",
    "    record.id = header[4]\n",
    "    record.name = header[3]\n",
    "    record.seq = record.seq.upper()\n",
    "    record.annotations = {\"target\": target, \"subtype\": header[0], \"country\": header[1]}\n",
    "    # Add it to the data table and update the target classes dictionary\n",
    "    targets[target] = targets.get(target, 0) + 1\n",
    "    data.append(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Properly fills the data table using the above function\n",
    "if scope == \"ALL\":\n",
    "    # Used to show progress\n",
    "    progress = ProgressBar()\n",
    "    # If scope is ALL, each filename is the name of each region used as a target class\n",
    "    for filename in progress(listdir(path)):\n",
    "        target = filename.split('.')[0]\n",
    "        for record in SeqIO.parse(path + \"/\" + filename, \"fasta\"):\n",
    "            add_record(record, target)\n",
    "    print(\"\")\n",
    "else:\n",
    "    # Else, countries are target classes, and the scope region is the filename\n",
    "    for record in SeqIO.parse(path + \"/\" + scope + \".fasta\", \"fasta\"):\n",
    "        target = record.id.split(\".\")[1]\n",
    "        add_record(record, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dipslay data information\n",
    "print(\"Data information:\")\n",
    "print(\"Number of sequences:\", sum(targets.values()))\n",
    "print(\"Number of targets:\", len(targets))\n",
    "print(\"Minimum number of instances:\", min(targets.values()))\n",
    "print(\"Maximum number of instances:\", max(targets.values()))\n",
    "\n",
    "# Dipslay data summary\n",
    "print(\"\\nData summary:\")\n",
    "for key, value in targets.items(): \n",
    "    print(\"Target:\", key, \"| Number of sequences:\", value)\n",
    "\n",
    "# Display the first 5 samples\n",
    "print(\"\\nInformation of the first 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(\"ID:\", data[i].id, \"| Sequence:\", data[i].seq[0:50], \"| Annotations:\", data[i].annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####      TRAIN / TEST DATA SPLIT       #####\n",
    "##############################################\n",
    "# Initialise train/test tables that will contain the data\n",
    "train_data = []\n",
    "test_data = []\n",
    "# Initialise train/test dictionaries that will contain the number of instances for each target\n",
    "test_split = {}\n",
    "train_split = {}\n",
    "# Initialise the dictionary with the targets keys and the value 0\n",
    "test_split = test_split.fromkeys(targets.keys(), 0)\n",
    "train_split = train_split.fromkeys(targets.keys(), 0)\n",
    "# Shuffle the data\n",
    "shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterate through the data\n",
    "for d in data:\n",
    "    # Get this records's target class\n",
    "    target = d.annotations[\"target\"]\n",
    "    # For sampling purposes: train/test threshold is based on n_samples if there is too much records for this target\n",
    "    threshold = min(targets[target], n_samples) * split_raito\n",
    "    # Until threshold for this target is reached, fills train data\n",
    "    if train_split[target] < threshold: \n",
    "        train_data.append(d)\n",
    "        train_split[target] += 1\n",
    "    # Then, fills test data (until eventually n_samples are collected)\n",
    "    elif test_split[target] < n_samples * (1-split_raito): \n",
    "        test_data.append(d)\n",
    "        test_split[target] += 1\n",
    "# Shuffle the data\n",
    "shuffle(train_data)\n",
    "shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary of the train/test split\n",
    "print(\"\\nTrain/Test split summary:\")\n",
    "for train_key, test_key in zip(train_split.keys(), test_split.keys()):\n",
    "    print(\"Target:\", train_key, \"| Train instances:\", train_split[train_key], \"| Test instances:\", test_split[test_key])\n",
    "print(\"\\nTotal number of training instances:\", len(train_data))\n",
    "print(\"Total number of testing instances:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "#####  FEATURES GENERATION BASED ON K-MERS   #####\n",
    "##################################################\n",
    "print(\"\\n         FEATURES GENERATION         \")\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary for the k-mers motifs features\n",
    "instances = {}\n",
    "# Used to show progress\n",
    "progress = ProgressBar()\n",
    "# Iterate through the training data\n",
    "for d in train_data:\n",
    "    # Go through the sequence \n",
    "    for i in range(0, len(d.seq) - k + 1, 1):\n",
    "        # Get the current k-mer motif feature\n",
    "        feature = str(d.seq[i:i + k])\n",
    "        # If it contains only the characters \"A\", \"C\", \"G\" or \"T\", it will be saved\n",
    "        if re.match('^[ACGT]+$', feature): \n",
    "            instances[feature] = 0\n",
    "    progress.update(len(instances))\n",
    "    # No need to keep going if motifs dictonary reaches max size\n",
    "    if len(instances) == 4 ** k:\n",
    "        break\n",
    "# Used to show progress\n",
    "progress.finish()\n",
    "# Save dictonary keys as biopython motifs object\n",
    "motifs = create(instances.keys())\n",
    "# Display the number of features\n",
    "print(\"\\nNumber of features:\", len(motifs.instances), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "##### GENERATION OF THE FEATURE MATRIX (x) AND TARGET VECTOR (y) #####\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate feature matrix and target vector\n",
    "def generateFeatures(data):\n",
    "    # Initialize the feature matrix\n",
    "    X = []\n",
    "    # Initialize the target vector\n",
    "    y = []\n",
    "    # Used to show progress\n",
    "    progress = ProgressBar()\n",
    "    # Iterate through the data\n",
    "    for d in progress(data):\n",
    "        # Generate an empty dictionary\n",
    "        x = {}\n",
    "        # Initialize the dictionary with targets as keys and 0 as value\n",
    "        x = x.fromkeys(motifs.instances, 0)\n",
    "        # Compute X (features matrix): the number of occurrence of k-mers (with overlaping)\n",
    "        for i in range(0, len(d.seq) - k + 1, 1):\n",
    "            feature = d.seq[i:i + k]\n",
    "            # Attempt to increment the number of occurrences of the current k-mer feature\n",
    "            try: x[feature] += 1\n",
    "            # It could fail because the current k-mer is not full ACGT\n",
    "            except: pass\n",
    "        # Save the features vector in the features matrix\n",
    "        X.append(list(x.values()))\n",
    "        # Save the target class in the target vector\n",
    "        y.append(d.annotations[\"target\"])\n",
    "    # Return matrices X and y (feature matrix and target vector)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate train/test feature matrices and target vectors\n",
    "x_train, y_train = generateFeatures(train_data)\n",
    "x_test, y_test = generateFeatures(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate feature matrix and target vector based on k-mer frequency, not the sum\n",
    "def generateFreqFeatures(x_sum):\n",
    "    X = []\n",
    "    for x in x_sum:\n",
    "        total = sum(x)\n",
    "        X.append(list(map((lambda i: i / total), x)))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Freq is ture, then the features matrix are frequency of k-mers, not their sum\n",
    "if freq:\n",
    "    x_train = generateFreqFeatures(x_train)\n",
    "    x_test = generateFreqFeatures(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####       FEATURES NORMALISATION       #####\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a MinMaxScaler between 0 and 1\n",
    "minMaxScaler = MinMaxScaler(feature_range = (0,1))\n",
    "# Apply a scaling to the train and test set\n",
    "x_train = minMaxScaler.fit_transform(x_train)\n",
    "x_test = minMaxScaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####         FEATURES SELECTION         #####\n",
    "##############################################\n",
    "print(\"\\n         FEATURES SELECTION          \")\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a linear model based on svm\n",
    "model = svm.SVC(C = 1.0, kernel='linear', class_weight = None)\n",
    "# Instantiate the RFE\n",
    "rfe = RFE(model, n_features_to_select = n_features, step = step, verbose=True)\n",
    "# Apply RFE and transform the training matrix\n",
    "x_train = rfe.fit_transform(x_train, y_train)\n",
    "# Tranform the test matrix (will be useed later for evaluation purposes)\n",
    "x_test = rfe.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the reduction percentage of the feature matrix\n",
    "reduction_percentage = ((len(motifs.instances) - n_features) / len(motifs.instances) * 100)\n",
    "# Print the reduction percentage\n",
    "print(\"\\nReduction percentage:\", round(reduction_percentage, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the table that will contain the selected features\n",
    "instances = []\n",
    "# Save selected k-mers features\n",
    "for i, mask in enumerate(rfe.support_): \n",
    "    if mask == True: instances.append(motifs.instances[i])\n",
    "# Save table as biopython motifs object\n",
    "features = create(instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####    TRAINING DATA VISUALISATION     #####\n",
    "##############################################\n",
    "print(\"\\n     TRAINING DATA VISUALISATION     \")\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the function to draw Scatter Plot\n",
    "def generateScatterPlot(title, figure_width, figure_height, data, X, y):\n",
    "    # If 2d dimensions\n",
    "    if n_components == 2:\n",
    "        # Initialize a 2-dimensional figure\n",
    "        fig, ax = plt.subplots(figsize=(figure_width, figure_height))\n",
    "    # If 3d dimensions\n",
    "    else:\n",
    "        # Initialize a 3-dimensional figure\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        ax = Axes3D(fig)\n",
    "    # List of markers\n",
    "    markers = [\"o\",\"+\", \"^\", \"x\"]\n",
    "    # List of colors\n",
    "    colors = [\"tab:blue\", \"tab:orange\", \n",
    "              \"tab:green\", \"tab:red\", \n",
    "              \"tab:purple\", \"tab:brown\", \n",
    "              \"tab:pink\", \"tab:grey\", \n",
    "              \"tab:olive\", \"tab:cyan\",]\n",
    "    \n",
    "    # Iterate through the targets\n",
    "    for i, target in enumerate(y):\n",
    "        # Set the list of axis positions\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        # If the number of targets is less than 10\n",
    "        if i < 10:\n",
    "            color = colors[i]\n",
    "            marker = markers[0]\n",
    "        # If the number of targets is less than 20\n",
    "        elif i < 20:\n",
    "            color = colors[i-10]\n",
    "            marker = markers[1]\n",
    "        # If the number of targets is less than 30\n",
    "        elif i < 30:\n",
    "            color = colors[i-20]\n",
    "            marker = markers[2]\n",
    "        # If the number of targets is less than 40\n",
    "        else:\n",
    "            color = colors[i-30]\n",
    "            marker = markers[3]\n",
    "            \n",
    "        # Iterate through the data\n",
    "        for i, d in enumerate(data):\n",
    "            # If the sequence belongs to the target of interest\n",
    "            if d.annotations[\"target\"] == target:\n",
    "                # Save the value of the positions\n",
    "                x.append(X[i][0])\n",
    "                y.append(X[i][1])\n",
    "                if n_components == 3: z.append(X[i][2])\n",
    "              \n",
    "        # Add the current scatter plot to the figure\n",
    "        if n_components == 2:\n",
    "            ax.scatter(x, y, c = color, label = target, alpha = 0.75, edgecolors = 'none', marker=marker)\n",
    "        else:\n",
    "            ax.scatter(x, y, z, c = color, label=target,alpha=0.75, edgecolors='none', marker=marker)\n",
    "\n",
    "    # Display the grid\n",
    "    ax.grid(True)\n",
    "    # Set the legend parameters\n",
    "    ax.legend(loc = 2, prop = {'size': 10})\n",
    "    # Set the tite\n",
    "    plt.title(title)\n",
    "    # Set axes labels\n",
    "    if n_components == 2:\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "    else: \n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.set_zlabel('PC3')\n",
    "    # Displqy the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a TSNE with 3 principal components\n",
    "tsne = TSNE(n_components = 3, perplexity = 50, verbose=True)\n",
    "# Apply TSNE to X_train\n",
    "x_tsne = tsne.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scatter plot of a TSNE\n",
    "generateScatterPlot(title= \"Scatter plot of a two-dimensional TSNE applied to the training data\", \n",
    "                    figure_width = 15, \n",
    "                    figure_height = 12, \n",
    "                    data = train_data, \n",
    "                    X = x_tsne, \n",
    "                    y = set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate PCA with 3 principal components\n",
    "pca = PCA(n_components = 3)\n",
    "x_pca =  pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scatter plot of a PCA\n",
    "generateScatterPlot(title= \"Scatter plot of a two-dimensional PCA applied to the training data\", \n",
    "                    figure_width = 15, \n",
    "                    figure_height = 12, \n",
    "                    data = train_data, \n",
    "                    X = x_pca, \n",
    "                    y = set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####   MODEL TRAINING AND PREDICTION    #####\n",
    "##############################################\n",
    "print(\"\\n    MODEL TRAINING AND PREDICTION    \")\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the train set\n",
    "model.fit(x_train, y_train)\n",
    "# Save the model to filename model_name\n",
    "joblib.dump(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict with model on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "# Display prediction\n",
    "print(\"Predictions (\" + str(len(y_pred)) + \"):\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####  MODEL PREDICTIONS VISUALISATION   #####\n",
    "##############################################\n",
    "print(\"\\n   MODEL PREDICTIONS VISUALISATION   \")\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will contain correct and incorrect data seq_records objects\n",
    "correct_data = []\n",
    "incorrect_data = []\n",
    "# Will contain correct and incorrect features vectors (just like x_test)\n",
    "correct_features = []\n",
    "incorrect_features = []\n",
    "# Iterate through test data\n",
    "for i, d in enumerate(test_data):\n",
    "    # Add an annotation to all test data stating its percentage range of ACGT characters\n",
    "    total_char = len(d.seq)\n",
    "    total_acgt = 0\n",
    "    for char in d.seq:\n",
    "        if re.match('^[ACGT]+$', char):\n",
    "            total_acgt += 1\n",
    "    acgt_percent = total_acgt / total_char\n",
    "    if acgt_percent >= 0.75: d.annotations[\"acgt-percent\"] = \"75-100\"\n",
    "    elif acgt_percent >= 0.50: d.annotations[\"acgt-percent\"] = \"50-75\"\n",
    "    elif acgt_percent >= 0.25: d.annotations[\"acgt-percent\"] = \"25-50\"\n",
    "    else: d.annotations[\"acgt-percent\"] = \"0-25\"\n",
    "    # Split test data into correct and incorrect sets depending on prediction results\n",
    "    if y_pred[i] == d.annotations[\"target\"]:\n",
    "        correct_data.append(d)\n",
    "        correct_features.append(x_test[i])\n",
    "    else:\n",
    "        # If it's incorrect, add the prediction class as an annotation\n",
    "        d.annotations[\"prediction\"] = y_pred[i]\n",
    "        incorrect_data.append(d)\n",
    "        incorrect_features.append(x_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification_report\n",
    "print(classification_report(y_test, y_pred, digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictonaries with pair of annotation -> number of incorrect records with this annotation\n",
    "subtypes = {}\n",
    "countries = {}\n",
    "predictions = {}\n",
    "acgt_percents = {}\n",
    "# Iterate through incorrect data\n",
    "for i in incorrect_data:\n",
    "    # Increment each kind of annotation with current record values as keys\n",
    "    subtypes[i.annotations[\"subtype\"]] = subtypes.get(i.annotations[\"subtype\"], 0) + 1\n",
    "    countries[i.annotations[\"country\"]] = countries.get(i.annotations[\"country\"], 0) + 1\n",
    "    predictions[i.annotations[\"prediction\"]] = predictions.get(i.annotations[\"prediction\"], 0) + 1\n",
    "    acgt_percents[i.annotations[\"acgt-percent\"]] = acgt_percents.get(i.annotations[\"acgt-percent\"], 0) + 1\n",
    "# Display number of incorrect records for each annotation, useful to spot any pattern here\n",
    "print(\"Incorrect predictions annotations:\")\n",
    "print(\"Subtype:\", subtypes)\n",
    "print(\"Country:\", countries)\n",
    "print(\"Prediction:\", predictions)\n",
    "print(\"ACGT percent:\", acgt_percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "matrix = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "# Build the heatmap\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.heatmap(matrix, \n",
    "            cmap = 'Blues', \n",
    "            annot = True, \n",
    "            fmt = \".0f\", \n",
    "            linewidth = 0.1, \n",
    "            xticklabels = targets.keys(), \n",
    "            yticklabels = targets.keys())\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show percentage of occurence of all features for all target classes in both train and test data\n",
    "matrix = []\n",
    "# Iterate through features\n",
    "for i, feature in enumerate(features.instances):\n",
    "    # Generate an empty dictionary\n",
    "    x = {}\n",
    "    # Initialize the dictionary with targets as keys and 0 as value\n",
    "    x = x.fromkeys(targets.keys(), 0)\n",
    "    # Count in all train data\n",
    "    for f, d in zip(x_train, train_data):\n",
    "        if f[i] > 0: x[d.annotations[\"target\"]] += 1\n",
    "    # Count in all test data\n",
    "    for f, d in zip(x_test, test_data):\n",
    "        if f[i] > 0: x[d.annotations[\"target\"]] += 1\n",
    "    # Vector of attendance percentage\n",
    "    vector = []\n",
    "    # Iterate through the number of instances and the number of occurrences\n",
    "    for n_instances, n_occurrences in zip(targets.values(), x.values()):\n",
    "        n_instances = min(n_instances, n_samples)\n",
    "        # Compute the percentage of k-mers attendance by target\n",
    "        attendance_percentage = 100 - ((n_instances - n_occurrences) / n_instances * 100)\n",
    "        # Save the attendance percentage in the specitic vector\n",
    "        vector.append(int(attendance_percentage))\n",
    "    # Save the vector of attendance percentage in the heatmap matrix\n",
    "    matrix.append(vector)\n",
    "# Build the heatmap\n",
    "fig, ax = plt.subplots(figsize=(15, 20))\n",
    "sns.heatmap(matrix, \n",
    "            annot = True, \n",
    "            fmt = \".0f\", \n",
    "            cmap = 'Blues',\n",
    "            linewidth = 0.1, \n",
    "            xticklabels = targets.keys(), \n",
    "            yticklabels = features.instances)\n",
    "plt.title(\"Percentage of presence of k-mers according to HIV subtypes\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute alignement of all incorrect records to all correct record and compute avegarge of scores\n",
    "print(\"\\nComparison of alignement scores between true and predicted class:\")\n",
    "ids = []\n",
    "matrix = []\n",
    "# Used to show progress\n",
    "progress = ProgressBar(max_value=len(incorrect_data[0:max_incorrect])*len(correct_data)).start()\n",
    "count = 0\n",
    "# Iterate through incorrect data\n",
    "for i in incorrect_data[0:max_incorrect]:\n",
    "    # Keep different averages for same target class and predicted target class of incorrect record\n",
    "    true_score_sum = 0\n",
    "    true_score_nb = 0\n",
    "    pred_score_sum = 0\n",
    "    pred_score_nb = 0\n",
    "    # Iterate through correct data\n",
    "    for c in correct_data:\n",
    "        # Compare only if both records are somewhat in the same category (both same subtype and acgt-percentage range)\n",
    "        if i.annotations[\"subtype\"] == c.annotations[\"subtype\"] and i.annotations[\"acgt-percent\"] == c.annotations[\"acgt-percent\"]:\n",
    "            # If this correct record is in the same class as current incorrect record\n",
    "            if i.annotations[\"target\"] == c.annotations[\"target\"]:\n",
    "                true_score_sum += pairwise2.align.globalxx(i.seq, c.seq, score_only=True)\n",
    "                true_score_nb += 1\n",
    "            # If this correct record is in the class that the current incorrect record has been predicted to\n",
    "            if i.annotations[\"prediction\"] == c.annotations[\"target\"]:\n",
    "                pred_score_sum += pairwise2.align.globalxx(i.seq, c.seq, score_only=True)\n",
    "                pred_score_nb += 1\n",
    "        # Used to show progress\n",
    "        count += 1\n",
    "        progress.update(count)\n",
    "    # Compute avergare only if similar correct records are found (avoid div per 0)\n",
    "    if true_score_nb != 0 and pred_score_nb != 0:\n",
    "        ids.append(i.id)\n",
    "        matrix.append([true_score_sum/true_score_nb, pred_score_sum/pred_score_nb])\n",
    "# Normalise results\n",
    "matrix = pd.DataFrame(np.array(matrix))\n",
    "matrix = matrix.div(matrix.max(axis=1), axis=0)\n",
    "# Build the heatmap\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(matrix, \n",
    "            #annot = True, \n",
    "            #fmt = \".0f\", \n",
    "            linewidth = 0.1,\n",
    "            cmap = 'Blues',\n",
    "            xticklabels = [\"True\", \"Prediction\"], \n",
    "            yticklabels = ids)\n",
    "plt.title(\"Comparison of alignement scores between true and predicted class\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For all incorrect records, compute average feature vectors of all correct records for both true and predicted classes\n",
    "for i_data, i_features in zip(incorrect_data[0:max_incorrect], incorrect_features[0:max_incorrect]):\n",
    "    # Both matrices to plot\n",
    "    true_features = []\n",
    "    pred_features = []\n",
    "    # Iterate through correct records\n",
    "    for c_data, c_features in zip(correct_data, correct_features):\n",
    "        # Compare only if both records are somewhat similar (either same subtype or acgt-percentage range)\n",
    "        if i_data.annotations[\"subtype\"] == c_data.annotations[\"subtype\"] or i_data.annotations[\"acgt-percent\"] == c_data.annotations[\"acgt-percent\"]:\n",
    "            # If this correct record is in the same class as current incorrect record\n",
    "            if i_data.annotations[\"target\"] == c_data.annotations[\"target\"]:\n",
    "                true_features.append(c_features)\n",
    "            # If this correct record is in the class that the current incorrect record has been predicted to  \n",
    "            if i_data.annotations[\"prediction\"] == c_data.annotations[\"target\"]:\n",
    "                pred_features.append(c_features)\n",
    "    # Compute avergare matrices only if similar correct records are found (avoid div per 0)\n",
    "    if len(true_features) != 0 and len(pred_features) != 0:\n",
    "        true_features_mean = np.array(true_features).mean(axis=0)\n",
    "        pred_features_mean = np.array(pred_features).mean(axis=0)\n",
    "        # Build the heatmap\n",
    "        fig, ax = plt.subplots(figsize=(40,5))\n",
    "        sns.heatmap([true_features_mean, i_features, pred_features_mean], \n",
    "                #annot = True, \n",
    "                #fmt = \".0f\", \n",
    "                linewidth = 0.1,\n",
    "                cmap = 'Blues',\n",
    "                xticklabels = features.instances,\n",
    "                yticklabels = [\"True\", \"Incorrect\", \"Prediction\"],)\n",
    "        plt.title(\"Comparaison of incorrect features vector with true and predicted features vectors averages\")\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For all incorrect records, compare apparence percentage of all correct records in both true and predicted classes\n",
    "for i_data, i_features in zip(incorrect_data[0:max_incorrect], incorrect_features[0:max_incorrect]):\n",
    "    # Dictionaries containing nb of occurences of features for all correct records\n",
    "    true_features = {}\n",
    "    pred_features = {}\n",
    "    true_features = true_features.fromkeys(features.instances, 0)\n",
    "    pred_features = pred_features.fromkeys(features.instances, 0)\n",
    "    true_total = 0\n",
    "    pred_total = 0\n",
    "    # Iterate through correct records\n",
    "    for c_data, c_features in zip(correct_data, correct_features):\n",
    "        # Compare only if both records are somewhat similar (either same subtype or acgt-percentage range)\n",
    "        #if i_data.annotations[\"subtype\"] == c_data.annotations[\"subtype\"] or i_data.annotations[\"acgt-percent\"] == c_data.annotations[\"acgt-percent\"]:\n",
    "        # If this correct record is in the same class as current incorrect record\n",
    "        if i_data.annotations[\"target\"] == c_data.annotations[\"target\"]:\n",
    "            true_total += 1\n",
    "            for value, key in zip(c_features, features.instances):\n",
    "                if value > 0: true_features[key] += 1\n",
    "        # If this correct record is in the class that the current incorrect record has been predicted to  \n",
    "        if i_data.annotations[\"prediction\"] == c_data.annotations[\"target\"]:\n",
    "            pred_total += 1\n",
    "            for value, key in zip(c_features, features.instances):\n",
    "                if value > 0: pred_features[key] += 1\n",
    "    # Compute avergare matrices only if similar correct records are found (avoid div per 0)\n",
    "    if true_total != 0 and pred_total != 0:\n",
    "        true_vector = list(map((lambda i: i / true_total), true_features.values()))\n",
    "        pred_vector = list(map((lambda i: i / pred_total), pred_features.values()))\n",
    "        # Build the heatmap\n",
    "        fig, ax = plt.subplots(figsize=(40,5))\n",
    "        sns.heatmap([true_vector, i_features, pred_vector], \n",
    "                #annot = True, \n",
    "                #fmt = \".0f\", \n",
    "                linewidth = 0.1,\n",
    "                cmap = 'Blues',\n",
    "                xticklabels = features.instances,\n",
    "                yticklabels = [\"True\", \"Incorrect\", \"Prediction\"],)\n",
    "        plt.title(\"Comparaison of incorrect features vector with true and predicted vectors of occurences percents\")\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tried something, did not work yet...\n",
    "\n",
    "#features = create([\"GGCGG\"])\n",
    "#for i in incorrect_data:\n",
    "#    graphic_features = []\n",
    "#    progress = ProgressBar()\n",
    "#    for pos, seq in progress(features.instances.search(i.seq)):\n",
    "#        graphic_features.append(GraphicFeature(start = pos, end= pos + k, strand = +1, color= \"#ffd700\", label=str(seq + \"\\n\" + \"Position : \" + str(pos))))\n",
    "#    record = GraphicRecord(sequence_length = len(i.seq), features=graphic_features)\n",
    "#    record.plot(figure_width = 15)\n",
    "#    plt.title(\"Sequence : \" + i.id) \n",
    "#    plt.show()\n",
    "#for c in correct_data:\n",
    "#    if c.annotations[\"target\"] == \"CRB\":\n",
    "#        graphic_features = []\n",
    "#        progress = ProgressBar()\n",
    "#        for pos, seq in progress(features.instances.search(i.seq)):\n",
    "#            graphic_features.append(GraphicFeature(start = pos, end= pos + k, strand = +1, color= \"#ffd700\", label=str(seq + \"\\n\" + \"Position : \" + str(pos))))\n",
    "#        record = GraphicRecord(sequence_length = len(i.seq), features=graphic_features)\n",
    "#        record.plot(figure_width = 15)\n",
    "#        plt.title(\"Sequence : \" + c.id) \n",
    "#        plt.show()\n",
    "#        break\n",
    "#for c in correct_data:\n",
    "#    if c.annotations[\"target\"] == \"OCE\":\n",
    "#        graphic_features = []\n",
    "#        progress = ProgressBar()\n",
    "#        for pos, seq in progress(features.instances.search(i.seq)):\n",
    "#            graphic_features.append(GraphicFeature(start = pos, end= pos + k, strand = +1, color= \"#ffd700\", label=str(seq + \"\\n\" + \"Position : \" + str(pos))))\n",
    "#        record = GraphicRecord(sequence_length = len(i.seq), features=graphic_features)\n",
    "#        record.plot(figure_width = 15)\n",
    "#        plt.title(\"Sequence : \" + c.id) \n",
    "#        plt.show()\n",
    "#        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}